{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tải và chuẩn bị dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Section 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# Đường dẫn đến dữ liệu.\n",
    "data_path = 'F:/dataML/data/TRX/output/TRX_18_26555370_20200628_061900_223181.csv'\n",
    "\n",
    "# Tên dataset\n",
    "dataset_name = os.path.splitext(os.path.basename(data_path))[0]\n",
    "\n",
    "# Đọc dữ liệu\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Successfully loaded data: {dataset_name}\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "# Convert OpenTime to datetime and set as index\n",
    "df['OpenTime'] = pd.to_datetime(df['OpenTime'])\n",
    "df.set_index('OpenTime', inplace=True)\n",
    "print(\"Data prepared with 'OpenTime' as index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lựa chọn thuộc tính và tách dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Y_TYPE = 'close_vs_maxhigh60_r8'\n",
    "features = [\n",
    "    'sma_macd_diff_5',\n",
    "    'sma_macd_diff_10',\n",
    "    'sma_macd_diff_sub',\n",
    "    'macd_diff',\n",
    "    'macd',\n",
    "    'rsi_14',\n",
    "    'ema_9', 'ema_25', 'ema_50',\n",
    "    'sma_9', 'sma_25', 'sma_50',\n",
    "    'Close'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[Y_TYPE]\n",
    "\n",
    "# Chia dữ liệu: 70% train, 20% val, 10% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2222, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale dữ liệu và cân bằng lớp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply SMOTE only on the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Original y_train class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\ny_train class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo chuỗi cho mô hình LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 60 # Using 60 minutes of data to predict\n",
    "\n",
    "# Create sequences for train, validation, and test sets\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_resampled, y_train_resampled, TIME_STEPS)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val.values, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, TIME_STEPS)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape} (samples, time_steps, features)\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng DataLoaders với PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "print(\"DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Định nghĩa mô hình LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện mô hình và Tối ưu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "input_size = X_train_seq.shape[2] # Number of features\n",
    "model = LSTMModel(input_size=input_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10 \n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "best_model_state_dict = None\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Giữ lại trạng thái model tốt nhất trong bộ nhớ thay vì lưu file\n",
    "        best_model_state_dict = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print('Validation loss decreased. Saving model state in memory...')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đánh giá mô hình trên tập Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_test = None\n",
    "if best_model_state_dict:\n",
    "    # Tải trạng thái model tốt nhất từ bộ nhớ\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            y_pred_list.extend(preds.cpu().numpy())\n",
    "    y_pred_test = np.array(y_pred_list).flatten()\n",
    "    \n",
    "    print(\"Predictions generated successfully from in-memory model.\")\n",
    "else:\n",
    "    print(\"Model was not trained or no best model state was saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Báo cáo phân loại và Biểu đồ Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if y_pred_test is not None:\n",
    "    print(\"\\n--- Section 9: Classification Report & Precision Plots ---\")\n",
    "    \n",
    "    # Độ chính xác (Accuracy)\n",
    "    accuracy = accuracy_score(y_test_seq, y_pred_test)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Báo cáo phân loại (Classification Report)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Use '1.0' as key for float-type labels, common in PyTorch tensors\n",
    "    report = classification_report(y_test_seq, y_pred_test, output_dict=True, zero_division=0)\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "\n",
    "    # Biểu đồ Precision, Recall, F1-Score cho nhãn 1\n",
    "    # Check for '1.0' (float) or '1' (str/int)\n",
    "    metrics_label_1 = report.get('1.0', report.get('1')) \n",
    "    if metrics_label_1:\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "            'Score': [metrics_label_1['precision'], metrics_label_1['recall'], metrics_label_1['f1-score']]\n",
    "        })\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x='Metric', y='Score', data=metrics_df)\n",
    "        plt.title('Precision, Recall, F1-Score for Label 1')\n",
    "        plt.ylabel('Score')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.savefig('precision_recall_f1_label_1.png')\n",
    "        print(\"\\nSaved Precision/Recall/F1 chart to 'precision_recall_f1_label_1.png'\")\n",
    "    else:\n",
    "        print(\"Could not generate precision plot for Label 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ma trận nhầm lẫn (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if y_pred_test is not None:\n",
    "    print(\"\\n--- Section 10: Confusion Matrix ---\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test_seq, y_pred_test)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    print(\"Saved confusion matrix heatmap to 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Độ quan trọng của Feature (Feature Importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if y_pred_test is not None:\n",
    "    print(\"\\n--- Section 11: Feature Importance (Permutation) ---\")\n",
    "    try:\n",
    "        importances = []\n",
    "        baseline_accuracy = accuracy_score(y_test_seq, y_pred_test)\n",
    "        \n",
    "        print(\"Calculating feature importance...\")\n",
    "        for i in range(X_test_tensor.shape[2]): # Iterate over each feature\n",
    "            temp_tensor = X_test_tensor.clone()\n",
    "            \n",
    "            # Permute data of the i-th feature\n",
    "            perm = torch.randperm(temp_tensor.size(0))\n",
    "            temp_tensor[:, :, i] = temp_tensor[perm, :, i]\n",
    "            \n",
    "            permuted_loader = DataLoader(TensorDataset(temp_tensor, y_test_tensor), batch_size=128)\n",
    "            \n",
    "            y_perm_preds = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, _ in permuted_loader:\n",
    "                    outputs = model(X_batch)\n",
    "                    preds = torch.sigmoid(outputs) > 0.5\n",
    "                    y_perm_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            perm_accuracy = accuracy_score(y_test_seq, np.array(y_perm_preds).flatten())\n",
    "            importance = baseline_accuracy - perm_accuracy\n",
    "            importances.append(importance)\n",
    "            print(f\"Importance of '{features[i]}': {importance:.4f}\")\n",
    "\n",
    "        # Sort and plot\n",
    "        sorted_indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=np.array(features)[sorted_indices], y=np.array(importances)[sorted_indices])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('Feature Importance (Permutation Method)')\n",
    "        plt.ylabel('Decrease in Accuracy')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        print(\"\\nSaved feature importance plot to 'feature_importance.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during feature importance calculation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trên tập dữ liệu mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if best_model_state_dict:\n",
    "    print(\"\\n--- Section 12: Testing on a New Dataset ---\")\n",
    "    \n",
    "    # THAY ĐỔI ĐƯỜNG DẪN TỚI FILE DỮ LIỆU MỚI CỦA BẠN\n",
    "    new_data_path = 'path/to/your/new_test_data.csv'\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading new dataset from: {new_data_path}\")\n",
    "        df_new = pd.read_csv(new_data_path)\n",
    "        df_new['OpenTime'] = pd.to_datetime(df_new['OpenTime'])\n",
    "        df_new.set_index('OpenTime', inplace=True)\n",
    "\n",
    "        # Áp dụng cùng một quy trình tiền xử lý\n",
    "        X_new = df_new[features]\n",
    "        y_new = df_new[Y_TYPE]\n",
    "        \n",
    "        # Sử dụng scaler đã được fit trên dữ liệu train\n",
    "        X_new_scaled = scaler.transform(X_new)\n",
    "        \n",
    "        # Tạo chuỗi\n",
    "        X_new_seq, y_new_seq = create_sequences(X_new_scaled, y_new.values, TIME_STEPS)\n",
    "        \n",
    "        # Chuyển sang tensor\n",
    "        X_new_tensor = torch.tensor(X_new_seq, dtype=torch.float32).to(device)\n",
    "        y_new_tensor = torch.tensor(y_new_seq, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        new_test_dataset = TensorDataset(X_new_tensor, y_new_tensor)\n",
    "        new_test_loader = DataLoader(new_test_dataset, batch_size=128)\n",
    "\n",
    "        # Lấy dự đoán cho dữ liệu mới\n",
    "        model.eval()\n",
    "        y_new_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in new_test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                y_new_preds_list.extend(preds.cpu().numpy())\n",
    "        \n",
    "        y_new_preds = np.array(y_new_preds_list).flatten()\n",
    "        \n",
    "        # In kết quả đánh giá\n",
    "        print(\"\\n--- Evaluation on New Dataset ---\")\n",
    "        new_accuracy = accuracy_score(y_new_seq, y_new_preds)\n",
    "        print(f\"Accuracy on new data: {new_accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report for new data:\")\n",
    "        print(classification_report(y_new_seq, y_new_preds, zero_division=0))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {new_data_path}. Please update the path and run again.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during testing on new data: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
